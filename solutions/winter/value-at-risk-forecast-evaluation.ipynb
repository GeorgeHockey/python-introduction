{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Value-at-Risk: Forecast Evaluation\n",
    "\n",
    "**Functions**\n",
    "\n",
    "`sm.OLS`, `stats.bernoulli`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 60\n",
    "    \n",
    "Compare this VaR to the HS VaR in the previous example.\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load Data\n",
    "sp500 = pd.read_hdf(\"./data/arch-data.h5\", \"sp500\")        \n",
    "eurusd = pd.read_hdf(\"./data/arch-data.h5\", \"eurusd\")\n",
    "\n",
    "sp500_returns = 100 * sp500.SP500.pct_change().dropna()\n",
    "eurusd_returns = 100 * eurusd.DEXUSEU.pct_change().dropna()\n",
    "\n",
    "with pd.HDFStore(\"./data/hs-var.h5\", mode=\"r\") as hdf:\n",
    "    sp500_hs = hdf.get(\"sp500_var\")\n",
    "    eurusd_hs = hdf.get(\"eurusd_var\")\n",
    "with pd.HDFStore(\"./data/fhs-var.h5\", mode=\"r\") as hdf:\n",
    "    sp500_fhs = hdf.get(\"sp500_var\")\n",
    "    eurusd_fhs = hdf.get(\"eurusd_var\")\n",
    "\n",
    "# Rename columns to distinguish\n",
    "sp500_hs.columns = [c.replace(\"VaR\",\"HS VaR\") for c in sp500_hs.columns]\n",
    "eurusd_hs.columns = [c.replace(\"VaR\",\"HS VaR\") for c in eurusd_hs.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rc(\"figure\", figsize=(16,12))\n",
    "plt.rc(\"font\", size=16)\n",
    "\n",
    "sp500_var = pd.concat([sp500_hs, sp500_fhs],axis=1)\n",
    "eurusd_var = pd.concat([eurusd_hs, eurusd_fhs], axis=1)\n",
    "\n",
    "for h in (1,5,10):\n",
    "    fig, axes = plt.subplots(2,1)\n",
    "    cols =[f\"{h}-day 5% VaR\", f\"{h}-day 5% HS VaR\"]\n",
    "    ax = sp500_var[cols].plot(ax=axes[0],legend=False)\n",
    "    ax.set_title(f\"Comparing {h}-day S&P 500 VaRs\")\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(\"% Value-at-Risk\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_xlim(sp500_var.index.min(), sp500_var.index.max())\n",
    "    \n",
    "    ax = eurusd_var[cols].plot(ax=axes[1], legend=False)\n",
    "    ax.set_title(f\"Comparing {h}-day EUR/USD VaRs\")\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(\"% Value-at-Risk\")\n",
    "    ax.set_xlim(eurusd_var.index.min(), eurusd_var.index.max())\n",
    "    ax.legend(frameon=False)\n",
    "    fig.tight_layout(pad=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "\n",
    "The HS VaRs are very smooth while the FHS VaRs are more dynamic.  The FHS VaRs are mostly driven by changes in volatility. The dynamics in the EUR/USD data are substantially different with long-swings evident in volatility."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 61\n",
    "Evaluate the FHS and HS VaR forecasts constructed in the previous exercises using:\n",
    "\n",
    "* HIT tests\n",
    "* The Bernoulli test for unconditionally correct VaR\n",
    "* Christoffersenâ€™s test for conditionally correct VaR\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct HITS\n",
    "cols =[\"1-day 5% VaR\", f\"1-day 5% HS VaR\"]\n",
    "combined = pd.concat([sp500_returns, sp500_var[cols]], axis=1).dropna()\n",
    "combined.columns = [\"ret\",\"fhs\",\"hs\"]\n",
    "hit_fhs = combined.ret < -combined.fhs\n",
    "hit_hs = combined.ret < -combined.hs\n",
    "hits = pd.DataFrame({\"hit_fhs\":hit_fhs,\"hit_hs\":hit_hs}).astype(\"float\")\n",
    "print(hits.mean())\n",
    "print(hits.corr())\n",
    "import numpy as np\n",
    "temp = hits.replace(0.0,np.nan)\n",
    "temp.iloc[:,0] += 0.05\n",
    "temp.iloc[:,1] -= 0.05\n",
    "temp.columns = [\"Filtered HS\", \"Historical Simulation\"]\n",
    "plt.rc(\"figure\", figsize=(16,6))\n",
    "ax = temp.plot(marker=\"o\",linestyle=\"none\",legend=False, markersize=12)\n",
    "ax.set_ylim(0.9,1.10)\n",
    "ax.set_yticks([])\n",
    "ax.set_ylabel(\"VaR Violations\")\n",
    "ax.set_xlabel(None)\n",
    "ax.legend(frameon=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "\n",
    "\n",
    "We start by constructing the HITs.  These are computed by comparing -1 times the VaRs to the returns. We previously aligned the VaRs so we don't need to shift them here. \n",
    "\n",
    "We see that both produce fewer HITs than they should, and that the FHS is particularly bad. The are mildly correlated with about half of the HITs being observed in the same period.The HS violation appear to be clustered in 2016 and 2019. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "phat = hits.mean(0)\n",
    "for col in hits:\n",
    "    hit = hits[col]\n",
    "    phat = hit.mean()\n",
    "    llf = stats.bernoulli(phat).logpmf(hit).sum()\n",
    "    llf0 = stats.bernoulli(0.05).logpmf(hit).sum()\n",
    "    lr = 2 * (llf - llf0)\n",
    "    pval = 1 - stats.chi2(1).cdf(lr)\n",
    "    print(f\"Method: {col} LR: {lr} P-value: {pval}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "This is the simplest test and only requires evaluating the Bernoulli log-likelihood at the MLE ($\\overline{HIT}$, the average) and at the value under the null (5%).  The test statistic is 2 times the difference in the two and has a $\\chi^2_1$ distribution.  Both models reject correct specification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in hits:\n",
    "    hit = hits[col]\n",
    "    hit_t = hit.shift(1)\n",
    "    hit_tp1 = hit\n",
    "    n00 = ((1-hit_t) * (1-hit_tp1)).sum()\n",
    "    n10 = (hit_t * (1-hit_tp1)).sum() \n",
    "    n01 = ((1-hit_t) * hit_tp1).sum()\n",
    "    n11 = (hit_t * hit_tp1).sum()\n",
    "    \n",
    "    p00_hat = n00 / (n00 + n01)\n",
    "    p11_hat = n11 / (n11 + n10)\n",
    "    p00 = p00_hat\n",
    "    p11 = p11_hat\n",
    "    llf = n00*np.log(p00) + n10 * np.log(1-p00) + n11 * np.log(p11) + n10 * np.log(1-p11)\n",
    "    \n",
    "    p11 = .05\n",
    "    p00 = 1 - p11\n",
    "    llf0 = n00*np.log(p00) + n10 * np.log(1-p00) + n11 * np.log(p11) + n10 * np.log(1-p11)\n",
    "    \n",
    "    lr = 2 * (llf - llf0)\n",
    "    pval = 1 - stats.chi2(2).cdf(lr)\n",
    "    print(f\"Christoffersen's test, Method: {col} LR: {lr} P-value: {pval}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "Christoffersen's test is also fairly simple.  We first construct the vectors of HITs at time t and t+1 to use the formula in the noted.  We then compute $n_{ij}$ for $i,j\\in\\{0,1\\}$. These are then used to estimate the model parameters which allow the log-likelihood to be computed for both the MLE and the null.  Two times this difference is the test statistic which has a $\\chi^2_2$ distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "results = {} \n",
    "for col in hits:\n",
    "    hit = hits[col] - 0.05\n",
    "    lags = [hit.shift(i+1) for i in range(5)]\n",
    "    lags = pd.concat(lags,axis=1)\n",
    "    var_col = \"fhs\" if \"fhs\" in col else \"hs\"\n",
    "    var = combined[var_col]\n",
    "    data = pd.concat([hit, var, lags], axis=1).dropna()\n",
    "    y = data.iloc[:,0]\n",
    "    x = sm.add_constant(data.iloc[:,1:])\n",
    "    x.columns = [\"const\",\"var\"] + [f\"hit_L_{i}\" for i in range(1,6)]\n",
    "    res = sm.OLS(y, x).fit()\n",
    "    r = np.eye(7)\n",
    "    joint = res.wald_test(r)\n",
    "    stat = np.squeeze(joint.statistic)\n",
    "    results[col] = {\"summary\": res.summary()}\n",
    "    results[col][\"stat\"] = f\"Stat: {stat}, P-value: {joint.pvalue}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"hit_fhs\"][\"summary\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_fhs\"][\"stat\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"hit_hs\"][\"summary\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_hs\"][\"stat\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "Finally we can run the dynamic quantile test (or HIT regression). The model estimated is\n",
    "\n",
    "$$ HIT_{t+1} = \\gamma_0 + \\gamma_1 VaR_{t+1|t} + \\sum_{i=1}^5 \\gamma_{1+i} HIT_{t-i+1} + \\epsilon_t+1 $$\n",
    "\n",
    "We construct the lags using `shift`.  Both models are rejected.  The HS model appears to have substantial \n",
    "serial correlation while the FHS as serial correlation, the wrong level, and excess sensitivity to the \n",
    "forecast VaR.\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct HITS\n",
    "cols =[\"10-day 5% VaR\", f\"10-day 5% HS VaR\"]\n",
    "rets_10 = 100 * sp500.SP500.pct_change(10)\n",
    "combined = pd.concat([rets_10, sp500_var[cols]], axis=1).dropna()\n",
    "combined.columns = [\"ret\",\"fhs\",\"hs\"]\n",
    "hit_fhs = combined.ret < -combined.fhs\n",
    "hit_hs = combined.ret < -combined.hs\n",
    "hits = pd.DataFrame({\"hit_fhs\":hit_fhs,\"hit_hs\":hit_hs}).astype(\"float\")\n",
    "print(\"Mean HIT %\")\n",
    "print(hits.mean())\n",
    "print(\"Correlation accross HITs\")\n",
    "print(hits.corr())\n",
    "import numpy as np\n",
    "temp = hits.replace(0.0,np.nan)\n",
    "temp.iloc[:,0] += 0.05\n",
    "temp.iloc[:,1] -= 0.05\n",
    "temp.columns = [\"Filtered HS\", \"Historical Simulation\"]\n",
    "ax = temp.plot(marker=\"o\",linestyle=\"none\",legend=False, markersize=12)\n",
    "ax.set_ylim(0.9,1.10)\n",
    "ax.set_yticks([])\n",
    "ax.set_ylabel(\"VaR Violations\")\n",
    "ax.set_xlabel(None)\n",
    "ax.legend(frameon=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "\n",
    "We construct the 10-day HITs using the VaR forecasts and the 10-day returns\n",
    "computed using `pct_change(10)`. The forecasts are already aligned and\n",
    "so the HITs are just violations. The plot shows that both models seem to have \n",
    "important problems.  The FHS model has a 2-year period with no HITs.  The HS\n",
    "forecast has 2 distinct period with no HITs. \n",
    "\n",
    "Both models produce close to the correct number of HITs and the HITs are\n",
    "fairly correlated accross the two models. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "horizon=10\n",
    "for col in hits:\n",
    "    hit = hits[col] - 0.05\n",
    "    lags = [hit.shift(i+horizon) for i in range(5)]\n",
    "    lags = pd.concat(lags,axis=1)\n",
    "    var_col = \"fhs\" if \"fhs\" in col else \"hs\"\n",
    "    var = combined[var_col]\n",
    "    data = pd.concat([hit, var, lags], axis=1).dropna()\n",
    "    y = data.iloc[:,0]\n",
    "    x = sm.add_constant(data.iloc[:,1:])\n",
    "    x.columns = [\"const\",\"var\"] + [f\"hit_L_{horizon+i}\" for i in range(1,6)]\n",
    "    bw = int(1.2 * y.shape[0] ** (2/5))\n",
    "    res = sm.OLS(y, x).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": bw})\n",
    "    r = np.eye(7)\n",
    "    joint = res.wald_test(r)\n",
    "    stat = np.squeeze(joint.statistic)\n",
    "\n",
    "    joint_ex = res.wald_test(r[1:,:])\n",
    "    stat_ex = np.squeeze(joint_ex.statistic)\n",
    "\n",
    "    results[col] = {\"summary\": res.summary()}\n",
    "    results[col][\"stat\"] = f\"Stat: {stat}, P-value: {joint.pvalue}\"\n",
    "    results[col][\"stat ex\"] = f\"Stat ex. constant: {stat_ex}, P-value ex. constant: {joint_ex.pvalue}\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"hit_fhs\"][\"summary\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_fhs\"][\"stat\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_fhs\"][\"stat ex\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"hit_hs\"][\"summary\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_hs\"][\"stat\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"hit_hs\"][\"stat ex\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "\n",
    "First note that the lagged hits start at 10 lags. This is necessary \n",
    "since everything on the right-hand side of the model must be known\n",
    "at time $t$, and lags 1,...,9 only happen in the future. \n",
    "\n",
    "The tests use `wald_test` and restrict all coefficient to be zero so\n",
    "that the loading matrix in $R\\hat{\\beta}$ is $I_k$ where $k$ is the number\n",
    "of variables in the model. I also included a test that ignores the intercept\n",
    "which uses the same $R$ excluding the first column. \n",
    "\n",
    "The regressions use Newey-West (Bartlett) covariance estimators since \n",
    "the data used to produce the HITs is overlapping.  \n",
    "\n",
    "Both models are rejected in the complete test.  The FHS model appears to\n",
    "have slightly less serial correlation althogh both test specifications \n",
    "reject the null that the VaR is correct. "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}